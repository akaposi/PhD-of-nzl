\chapter{Type Theory}
\label{bg}

Type theory is usually used to refer to a formal systems in which every term always has its type. It was initially invented as a foundation of mathematics as an alternative to set theory, and later works well in computer science as programming languages which we can write certified programs with type system. There are a variety of type theories, like Russell's theory of types, simply typed calculus, Gödel's System T \cite{gdl:1931} etc. In this thesis we mainly focus on Per Martin-L\"{o}f's intuitionistic type theory (N.B.\ we will use "Type Theory'' specially for it if not ambiguous). There are also different versions of \mltt and the intentional version (\itt for short) has better computational behavior and widely used in programming languages like Agda, Epigram etc. However several desirable extensional concepts such as functional extensionality and quotient types are not available in \itt. Much research has been done to extend type theory with these concepts and new interpretations of type theory are popular and reasonable solutions. \hott is one of them and is also an variant of \mltt and connected with homotopy theory. 

In this chapter we will first briefly introduce the original motivation and evolution of type theory. Then we mainly explain important notions in \mltt, and a list of extensional concepts will be presented. Finnally we will also introduce the main programming language Agda which is implementation of intensional version of \mltt.


\section{A brief history of Type Theory}

Type theory was firstly introduced as an refinement of set theory. 
In the 1870s, George Cantor and Richard Dedekind founded set theory as a branch of mathematical logic and started to use set theory as a language to describe definitions of various mathematical objects.
In the 1900s, Bertrand Russell discovered a paradox in this system. In the naïve set theory, there was no distinction between small sets like the set of natural numbers and "larger" sets like the set of all sets.

\begin{example}[Russell's Paradox]
Let $R$ be the set of all sets which do not contain themselves
$R = \{x ~| ~x \not\in  x\}$
Then we got a cotradiction
$R \in R \iff R \not\in R$
\end{example}

To avoid this paradox, Russell and Frege found that they have to make an distinction between objects, predicates, predicates of predicates, etc. Then Russell proposed the theory of types \cite{rus:1903} where the distinction is internalised by types.
In this simple type theory, Each mathematical object is assigned a type. This is done in a hierarchical structure such that "larger" sets and small sets reside in different levels. The "set" of all
sets is no longer a small set, hence the
paradox disappears.

The elementary notion of type theory \emph{type} plays a similar role to set in set theory, but differs fundamentally. Every term comes with its unique type while in set theory, an element can belong to multiple sets.
For example to state a term of natural number $2$, we have to use a typing judgment $2:\N$, where $\N$ is the set of natural numbers. The term are usually constructed using a list of constructors binding to its type. Hence a term of integer $2 : \Z$ is constructively different term in type theory, while it is not the case in set theory.

Following the idea of theories of types, various type theories have been developed since then. 
Simply typed lambda calculus (or Church's theory of types) is the first type theory to introduce functions as primitive objects \cite{sep-type-theory}. It was originally introduced by Alonzo Church in 1940 to avoid his
the Kleene-Rosser paradox\cite{kleene1935inconsistency} in his untyped lambda calculus.

\begin{example}[Kleene-Rosser paradox]
Suppose we have a function $f = \lambda x . \neg (x ~ x) $, then we can deduce a contradiction by applying it to itself:

$f f = (\lambda x . \neg (x ~ x)) f = \neg (f ~ f)$
\end{example}

It is applied to various fields including computer science. For instance, Haskell was originally based on one of the variants of lambda calculus called System F\footnote{It has evolved into System FC recently}. 


%There are also other refinements of lambda calculus which is illustrated by the $\lambda$-cube \cite{barendregt1991introduction}.

In 1970s, Per Martin-L\"{o}f \cite{per:71,per:82} developed his profound intuitionistic type theory (also called \mltt or simply, \emph{Type Theory}).
It serves as a foundation of constructive mathematics \cite{martin1984intuitionistic} and also can be encoded as functional programming languages \cite{DBLP:dblp_journals/tcs/Troelstra99} in
which the evaluation of a well-typed program always terminates \cite{nor:90}. 


From early type theories like Russell's and Church's to modern type theories like de Bruijn's Automath, \mltt and Coquand's Calculus of Constructions (CoC), one of the most important extension and discovery is the correspondence between mathematic proofs and computer programs (terms).
Different to set theory whose axioms are based on first-order logic, in modern type theories, intuitionistic logic concepts can be encoded as types through the 
\textbf{Curry-Howard isomorphism (correspondence)}.
The American mathematician Haskell Curry and logician William Alvin Howard first discovered a correspondence between logic and computation. They found that mathematical proofs or generally propositions can be encoded as types and proofs can be given by constructing terms (programs). The idea also relates to the Brouwer–Heyting–Kolmogorov (BHK) interpretation of intuitionistic logic. For example, a proof of $P \wedge Q$ can be encoded as a product type $P \times Q$ which contains a proof of $p : P$ and a proof of $q : Q$. Computationally, implications are function types, conjunctions are product types, true is unit type, false is empty type etc. 
With dependent types, the correspondence extended to predicated logic: the universal and existential quantification correspond to dependent functions and dependent sums. 
This feature turns Type Theory into a programming language where we can formalise proofs as computer programs. We can do computer aid reasoning about mathematics as well as programs. From a programmer's perspective, it provides a programming language where we can write certified programs.

% also cite http://www.cs.ru.nl/~herman/onderwijs/provingwithCA/paper-lncs.pdf}
% cite http://plato.stanford.edu/entries/type-theory/

Another central concept in \mltt is \textbf{Dependent types}\label{dpty}.
A dependent type is a type depends on values of other types \cite{dtw}. It provides us the meaning to defining families of types, for example, a family of lists with explicit length called \emph{Vector}, for example $\text{Vec} ~\N ~3$ stands for a three elements list of $\N$. Since there is more information in the type, the program specifications can be expressed by accurately. In the example of vectors, we can write a look-up function without "index out of range" problems. It is much simpler to write multiplication of matrix with depend types.


The first version of \mltt is 71's formulation \cite{per:71} which is impredicative and turned our to be inconsistent due to Girard's paradox \cite{hurkens1995simplification}. It is impredicative in the sense that the universe of types is impredicative. The notion of a \textbf{universe of types} was first used by Martin-L\"{o}f \cite{Martin-Lof-1973} to describe the type of all types and usually denoted as $\mathsf{U}$. An impredicative universe $\mathsf{U}$ has an axiom $\mathsf{U} : \mathsf{U}$.
Starting from his 72's version \cite{Martin-Lof-1972}, a predicative hierarchy of universes is adopted and they are more widely used. Briefly speaking, it starts from a universe of small types called $U_0$ and for each $n : \N$ we have $U_n : U_{n+1}$ which forms a cumulative hierarchy of universe. There is a more detailed introduction to the notion of universe written by Erik Palmgren \cite{Palmgren98onuniverses}.


\textbf{Equality} is always one of the most contentious topic in Type Theory.
In regular mathematics the notion of equality are usually used to describe sameness and taken as a given.
But in Type Theory, we have different notions of equality or equivalence of the terms.
First of all, \textbf{definitional equality} is a judgment-level equality, e.g.\  $a \equiv b$, which holds when two terms have the same normal forms\cite{nor:90}. It is also called \textbf{judgemental equality} \cite{martin1984intuitionistic}. Usually it already includes \textbf{computational equality} which is the congruence on terms generated from reduction rules like $\beta$-reduction and $\eta$-reduction. 

In 72's version of \mltt, there is a recursively defined equalities for given types. It is a function decides whether two terms are equal by pattern match and the result is either unit type or empty type.

Since equalities are also propositions, they can be encoded as types. \textbf{Propositional equality} is an equality between two expressions of the same type which can be proved to be computationally equal in each case. It is also called \textbf{intensional equality} \cite{nor:90} because it arises from construction. In 73's version \cite{Martin-Lof-1973} he first introduced an inductively defined identity types to interalising equality as types, for example $\text{Id}_{\N}(a+b, b+a)$ (or $a = b$).


A type theory is \textbf{intensional} if we propositionally equal terms are not definitionally equal or is extensional if it accepts \textbf{equality reflection rule}.
\begin{equation}
\label{reflection}
\infer[\text{Reflection}]{a \equiv b}{a = b}
\end{equation}

Note, the other way around is trivial.

In \itt, like the 73's version or Agda, propositional equality is different to definitional equality. 
The definitional equality is always decidable hence type checking that depends on definitional equality is
decidable as well~\cite{alti:lics99}.
In \ett, like the 80's version \cite{martin1984intuitionistic} or NuPRL, propositional equality is undistinguished with definitional equality, in other words, two propositional equal objects are judgmentally equal. Hence the type checking has to respect propositional equality which are not decidable in general.
In practice \itt is more widely used because type checking is decidable and terminated. Languages such as Coq, Agda and Epigram are all intensional.

However in \itt, \textbf{extensional equality} which is the equality arising from external properties is usually not available. For example equality of functions, equality of different proofs for the same proposition, and more extensional concepts like quotient types are desirable and reasonable to acquire. However simply axiomaising them makes Type Theory non-canonical which we will shown in the later discussion of extensional concepts.


To add these extensional concepts into \itt without losing decidable type checking and canonicity, it seems that types have to be interpreted with more complicated structures rather than sets to internalise equality as higher structures.
In 1990s, Hofmann and Altenkirch invented different models of type theory such as setoid models and groupoid models.

%It is one of the most important topics in Type Theory.

%Altenkirch and McBride also introduced a variant of \ett called
%\emph{Observational Type Theory}  \cite{alt:06} in which definitional equality is
%decidable and propositional equality is extensional.

%cite http://adam.chlipala.net/cpdt/html/Equality.html

In recent years, a new interpretation of intensional \mltt by homotopy-theoretical notions \cite{voe:06} gives rise to \hott which is an univalent foundation of mathematics. 
Type are treated as spaces or higher groupoids, and terms are points of this space, and more generally, functions between types are \emph{continuous maps}. Identity types are \emph{paths}, identity types of identity types are \emph{homotopies}. Although these notions are originally defined with topological bases, we only treat them purely homotopically. The equality is internalised into types such that the types have infinite level of higher structures as \wog.


The new interpretation clarifies the nature of equality in Type Theory.
The central idea of \hott is \text{univalence} which can be understood as isomorphic types are equal.
In regular Mathematics we usually do abstract reasoning on structures which applied to all isomorphic structures, because they can not be distinguished by other objects, hence isomorphic structures can be identitfied, Univalence can be seen as a formal acceptance of this idea in Type Theory. 
The univalence enables us to do abstract reasoning about types, and many extensional concepts arise automatically from it. It also helps mathematicians to reason about homotopy theory in programming languages.

It is a crucial topic to build a model of dependent types theory where univalence has a computational interpretation so that the good computational properties of type theory is preserved \cite{bezem2013model}. There are different models based on simplicial sets, cubical sets and syntactic approach to define \wog in \itt which the author has done some work in this direction in chapter~\ref{wog}.

Following is a list of different versions of \mltt,

\begin{enumerate}

\item The 71's formulation \cite{per:71} is impredicative and turned our to be inconsistent due to Girard's paradox.

\item The 72's version which is published in 1996 \cite{Martin-Lof-1972} abandons the impredicative universe and all later versions are predicative. It does not have inductive identity types but recursively defines equalities for given types.

\item The 73's version \cite{Martin-Lof-1973} firstly introduced inductively defined identity types to interalising equality as types. 

\item The 80's version which is summarised by Giovanni Sambin in 1984 \cite{martin1984intuitionistic} is extensional. It adopts equality reflection, namely an inhabitant of an identity type implies definitionally equality.

\item In the homotopic version \cite{hott}, Vladimir Voevodsky extends it with univalence axiom and provides a homotopic interpretation of it.

\end{enumerate}



\section{The Formal System of Type Theory}

The formal type system of type theory is given by a list of judgments and a sequence of rules written as derivation of these judgments. Usually we have following judgments in this thesis:


\begin{tabular}{l l}
$\Gamma \vdash$ & $\Gamma$  is a well formed context \\
$\Gamma \vdash A$ & $A$  is a well formed type \\
$\Gamma \vdash a : A$ & $a$ is a well typed term of type $A$ in context $\Gamma$ \\
$\delta : \Gamma \Rightarrow \Delta$ & $\delta$ is a substitution from context $\Gamma$ to $\Delta$ \\
\end{tabular}

There are also equality judgments for contexts, types, terms and substitution. For instance,

$\Gamma \vdash a \equiv a' : A$  ~ $a$ and $a'$ are definitionally equal terms of type $A$ in context $\Gamma$

In \itt the judgmental equality $\equiv$ is the same as definitional equality, while propositional equality is usually expressed by an inhabitant of the identity type $\Gamma \vdash p: a =_{A} a' $.



The rules usually decribes how we can define types with judgements above. They are syntactic rules but the semantic meaning may be revealed from the construction.
First of all, a \textbf{context} is either empty (denoted as $()$) or extended by context comprehension:

\infrule[comprehension]{\Gamma \vdash \andalso \Gamma \vdash A}{\Gamma,A \vdash }

In practice, the empty context is usually left out, for example $\vdash \bot$.



Rules for types are usually classified as formation rule, introduction rule, elimination rule, computation rule ($\beta$) or uniqueness rule ($\eta$). Here we will only show several fundamental types.

The function type and product type are generalised as \textbf{$\Pi$-types} (dependent function type) and \textbf{$\Sigma$-types} (dependent product type).

\begin{multicols}{2}
\infrule[$\Pi$-form]{\Gamma \vdash A \andalso \Gamma , A \vdash B}{\Gamma \vdash \Pi ~A ~B }
\columnbreak
\infrule[$\Pi$-intro]{\Gamma , A \vdash b : B}{\Gamma \vdash \lambda (x:A). b : \Pi ~A ~B }
\end{multicols}

\infrule[$\Pi$-elim]{\Gamma \vdash f : \Pi ~ A~B \andalso \Gamma \vdash a : A}{\Gamma \vdash f(a): B[a/x]}


\begin{multicols}{2}
computation rule ($\beta$)
$$(\lambda (x:A).b)(a) \equiv b[a/x]$$

\columnbreak

uniqueness rule ($\eta$)
$$f \equiv \lambda x. f(x) $$
\end{multicols}

In the expressions like $\lambda (x:A).b$, it binds free occurrences of $x$ in $b$.




\begin{multicols}{2}
\infrule[$\Sigma$-form]{\Gamma \vdash A \andalso \Gamma , A \vdash B}{\Gamma \vdash \Sigma ~A ~B }
\columnbreak
\infrule[$\Sigma$-intro]{\Gamma \vdash a : A \andalso \Gamma \vdash b : B[a]}{\Gamma \vdash (a,b) : \Sigma ~A ~B}
\end{multicols}

There are two ways to eliminate a term of $\Sigma$-types,


\begin{multicols}{2}
\infrule[$\Sigma$-proj$_1$]{\Gamma \vdash t : \Sigma ~ A ~B}{\pi_1~t : A}
\columnbreak
\infrule[$\Sigma$-proj$_2$]{\Gamma \vdash t : \Sigma ~ A ~B}{\pi_2~t : B[\pi_1~t]}
\end{multicols}

The computation rule is, $\pi_1 ~(a,b) \equiv a$ and $\pi_2 ~(a,b) \equiv b$

And the uniqueness rule is,

$$t \equiv (\pi_1 ~t, \pi_2 ~t)$$

\textbf{Identity type} is introduced by Martin-L\"{o}f \cite{nor:90} to encode propositionally equality, it is homogeneous and internalising definitional equality.


\begin{multicols}{2}
\infrule[Id-form]{\Gamma \vdash A}{\Gamma , a : A, a' : A \vdash \text{Id}_A(a, a')}
\columnbreak
\infrule[Id-intro]{\Gamma \vdash A}{\Gamma , a : A \vdash r(a) : \text{Id}_A(a, a)}
\end{multicols}

The subscript $A$ is sometimes omitted and sometimes we will also write $a =_{A} a'$ or simply $a = a'$ for identity type.

\infrule[Id-J]{\Gamma , a : A, a' : A, p : \text{Id}_A(a,a') \vdash C(a,a',p)  \andalso \Gamma , a : A \vdash t : C(a,a,r(a))}{\Gamma, a: A, a': A,p : \text{Id}_A(a,a') \vdash J(t,a,a',p): C(a',a',p)}

Its computation rule is,

$$J(t,a,a,r(a)) = t$$


There is another eliminator $K$ which is not accepted in general because it relies on the uniqueness of identity proof.

\infrule[Id-K]{\Gamma \vdash a : A \andalso \Gamma, p : \text{Id}_{A}(a,a) \vdash C(a) \andalso \Gamma \vdash t : C(a)}{\Gamma, q: \text{Id}_A(a,a)	\vdash K(q)}


The unique rule of identity type is not usually included. We will consdered higher structures of identity types and only if a type has a structure of set in the sense that its equality is propositional, we have uniquess not identity proof (UIP) and use K eliminator.

\textbf{Unit type}


\begin{multicols}{2}
\infrule[Unit-form]{}{\vdash \textbf{1}}
\columnbreak
\infrule[Unit-intro]{}{\vdash * : \textbf{1}}
\end{multicols}

\infrule[Unit-elim]{\Gamma ,x : \textbf{1} \vdash A \andalso \Gamma \vdash t : A[*/x] }{ \vdash t : A}


\textbf{Empty type}
There is no term for empty type so there is no introudction rule.


\begin{multicols}{2}
\infrule[Empty-form]{}{\vdash \textbf{0}}

\columnbreak

\infrule[Empty-elim]{\Gamma \vdash A}{\Gamma, e:\textbf{0} \vdash \text{abort}(e) : A}

\end{multicols}

\textbf{Univese types}


\begin{multicols}{2}

\infrule[$\mathsf{U}$-form]{}{\Gamma \vdash \mathsf{U}}


\columnbreak

\infrule[$\mathsf{U}$-El]{\Gamma \vdash \hat{A} : \mathsf{U}}{\Gamma \vdash \mathsf{El}({\hat{A}})}

\end{multicols}

The notation of $\hat{A}$ indicates that it is a code for a type rather than a type.


\textbf{Inductive types}

Some systems of Type Theory provide an inductive way to define types.

\begin{definition}\label{df:inductivetypes}
\textbf{Inductive types}.

Inductive types is a self-referential schema to define new types by specifying a collection of \emph{constructors} which can be constants and functions.
\end{definition}

The formation and introduction rules of a type is enough to build a type inductively. Examples like
natural numbers can be defined as follows:

\begin{itemize}
\item $0 : \N$
\item $\text{suc} : \N \rightarrow \N$
\end{itemize}

The terms are freely generated by a finite list of constructors, for instance, $\text{suc} (\text{suc} 0)$ is $2$. They are similar to data structures in programming languages, so most systems of Type Theory used as programming languages have inductive types along with structural recursion to eliminate or use them.

\section{An implementation of \mltt: Agda}

Agda is a dependently typed functional programming language which is designed based on intensional version
of \mltt \cite{agdawiki:main}. 

\begin{itemize}

\item \textit{Functional programming language}. As the name indicates that, functional programming languages emphasizes the application of functions rather than changing data in the imperative style like C{}\verb!++! and Java. The base of functional programming is lambda calculus. The key motivation to develop functional programmming language is to eliminating the side effects which means we can ensure the result will be the same no matter how many times we input the same data. There are several generations of functional programming languages, for example Lisp, Erlang, Haskell etc. Most of the applications of them are currently in the academic fields, however as the functional programming developed, more applications will be explored.

\item \textit{Dependent type}. 
In Haskell and other Hindley-Milner style languages, types and values are clearly distinct \cite{tutorial}, but in Agda, the border between types and values is vague. Types can depend on values of other types \cite{dtw} so that predicated logic is available and programs can be specified more sophisticatedly. We can write less error-prone programs because it is possible to write better specification which leaves the work of condition checking to the type checker, for example the multiplication of matrix.


\item \textit{Implementating Per Martin-Löf Type Theory}. It is a variant of \itt which is based on the Curry-Howard isomorphism\cite{aboa}. It means that we can reason about Mathematics and programs by constructing proofs as programs. Usually the correctness of programs has to be verified on the meta-level. However in Agda we verify programs within itself and express specifications and  programs at the same time as Nordström et al. \cite{nps} pointed out.
\end{itemize}




Some features of being a functional programming language features makes theorem proving easier,

\begin{itemize}
\item \textit{Pattern matching}. The mechanism for dependently typed pattern matching is very powerful \cite{alti:pisigma-new}. Pattern matching is a more intuitive way to use terms compared to the eliminators. For example, by pattern matching on a term of identity type, the two sides on the equation automatically becomes the same in the context so that we can simply prove the symmetry of identity as,

\begin{code}
\\
\>\AgdaFunction{symm} \AgdaSymbol{:} \AgdaSymbol{\{}\AgdaBound{A} \AgdaSymbol{:} \AgdaPrimitiveType{Set}\AgdaSymbol{\}\{}\AgdaBound{a} \AgdaBound{b} \AgdaSymbol{:} \AgdaBound{A}\AgdaSymbol{\}} \AgdaSymbol{→} \AgdaBound{a} \AgdaDatatype{≡} \AgdaBound{b} \AgdaSymbol{→} \AgdaBound{b} \AgdaDatatype{≡} \AgdaBound{a}\<%
\\
\>\AgdaFunction{symm} \AgdaInductiveConstructor{refl} \AgdaSymbol{=} \AgdaInductiveConstructor{refl}\<%
\\
\end{code}

compared to the case of using eliminator J,

\begin{code}\label{symmetry}
\\
\>\AgdaFunction{symm'} \AgdaSymbol{:} \AgdaSymbol{\{}\AgdaBound{A} \AgdaSymbol{:} \AgdaPrimitiveType{Set}\AgdaSymbol{\}\{}\AgdaBound{a} \AgdaBound{b} \AgdaSymbol{:} \AgdaBound{A}\AgdaSymbol{\}} \AgdaSymbol{→} \AgdaBound{a} \AgdaDatatype{≡} \AgdaBound{b} \AgdaSymbol{→} \AgdaBound{b} \AgdaDatatype{≡} \AgdaBound{a}\<%
\\
\>\AgdaFunction{symm'} \AgdaSymbol{=} \AgdaFunction{J} \AgdaSymbol{(λ} \AgdaBound{a} \AgdaBound{b} \AgdaBound{\_} \AgdaSymbol{→} \AgdaBound{b} \AgdaDatatype{≡} \AgdaBound{a}\AgdaSymbol{)} \AgdaSymbol{(λ} \AgdaBound{\_} \AgdaSymbol{→} \AgdaInductiveConstructor{refl}\AgdaSymbol{)} \AgdaSymbol{\_} \AgdaSymbol{\_}\<%
\\
\end{code}

\item \textit{Inductive \& Recursive definition}. In Agda, types are often defined inductively, for example, natural numbers is defined as

\begin{code}\>\<%
\>\AgdaKeyword{data} \AgdaDatatype{ℕ} \AgdaSymbol{:} \AgdaPrimitiveType{Set} \AgdaKeyword{where}\<%
\\
\>[0]\AgdaIndent{2}{}\<[2]%
\>[2]\AgdaInductiveConstructor{zero} \AgdaSymbol{:} \AgdaDatatype{ℕ}\<%
\\
\>[0]\AgdaIndent{2}{}\<[2]%
\>[2]\AgdaInductiveConstructor{suc} \<[7]%
\>[7]\AgdaSymbol{:} \AgdaSymbol{(}\AgdaBound{n} \AgdaSymbol{:} \AgdaDatatype{ℕ}\AgdaSymbol{)} \AgdaSymbol{→} \AgdaDatatype{ℕ}\<%
\>\<\end{code}

The function for inductive types are usually recursively defined using pattern matching. For example
\begin{code}\>\<%
\\
\>\AgdaFunction{double} \AgdaSymbol{:} \AgdaDatatype{ℕ} \AgdaSymbol{→} \AgdaDatatype{ℕ}\<%
\\
\>\AgdaFunction{double} \AgdaInductiveConstructor{zero} \AgdaSymbol{=} \AgdaInductiveConstructor{zero}\<%
\\
\>\AgdaFunction{double} \AgdaSymbol{(}\AgdaInductiveConstructor{suc} \AgdaBound{n}\AgdaSymbol{)} \AgdaSymbol{=} \AgdaInductiveConstructor{suc} \AgdaSymbol{(}\AgdaInductiveConstructor{suc} \AgdaSymbol{(}\AgdaFunction{double} \AgdaBound{n}\AgdaSymbol{))}\<%
\>\<\end{code}

It also enables programmers to prove propositions in the same manner of mathematical induction and case analysis.

\item \textit{Lazy evaluation}. Lazy evaluation could eliminate unecessary operation to delay a computation until we need its result. It is often used to handle infinite data structures. \cite{wiki:Lazy_evaluation}

\end{itemize}

Compared to other programming languages like Haskell, there is an interactive emacs interface which provides a few important functions.

\begin{itemize}
\item \textit{Type checker}. Type checker is an essential part of Agda. When a file is loaded in Agda-mode, it will detect type mismatch problems without compiling. It helps programmer build programs interactively by presenting the context information of given incomplete hole in the code.
It also includes a coverage checker and a termination checker.
\emph{Coverage checker} makes sure that the patterns cover all possible cases \cite{aboa}. 
\emph{Termination checker} will detect possible non-termination. The missing cases error will be reported by type checker. The suspected non-terminated definition can not be used by other ones. All programs must terminate in Agda so that it will not crash \cite{tutorial}.  The type checker then ensures that the proof is complete and not yet been proved by itself. 
In Agda, type signatures for functions are essential for most definitions due to the presence of type checker.
 
\item \textit{Interactive interface}. It has a emacs-based interface for interactively writing and verifying proofs.
As long as code is loaded, namely type checked, the code will be highlighted and problematic code is colored by red for non-terminating and yellow for not inferable. In the interactive emacs, there are a few convenient shortcut keys, for example showing the context, refining goal with partial program, navigating to definitions of some codes. The refinement function helps us incrementally build programs with explict context information.
The code navigation alleviates a great deal of work of programmers to look up the library.


\item \textit{Unicode and mixfix support}. In Haskell and Coq, unicode support is not an essential part. The name of operations can be very complicated without enough symbols. However in Agda, it provides unicode inputs and reads unicode symbols like $\beta$, $\forall$ and $\exists$. 
It also supports mixfix operators like $\_+\_$ and $\_-\_$ so that we have a more flexible way to define a better notation. For example, in the following theorem about commutativity of plus for natural numbers, $\AgdaDatatype{ℕ}$ and $\AgdaDatatype{≡}$ are unicodes, $\AgdaFunction{+}$ and $\AgdaDatatype{≡}$ are mixfix operators ($\AgdaFunction{+}$ is not primitive operator in Agda).

\begin{code}
\>\AgdaFunction{comm} \AgdaSymbol{:} \AgdaSymbol{∀} \AgdaSymbol{(}\AgdaBound{a} \AgdaBound{b} \AgdaSymbol{:} \AgdaDatatype{ℕ}\AgdaSymbol{)} \AgdaSymbol{→} \AgdaBound{a} \AgdaFunction{+} \AgdaBound{b} \AgdaDatatype{≡} \AgdaBound{b} \AgdaFunction{+} \AgdaBound{a}\<%
\end{code}

The unicode and mixfix improves the readability and provides familiar symbols used in Mathematics.
Interestingly we could use some other languages characters to define functions such as chinese characters.


\item \textit{Implicit arguments and wildcards}. Sometimes an argument is unnecessary to state. If an argument can be infered from other arguments we can mark it as implicit with curly brackets in the type signature. For example, whenever we give an argument $\mathsf{a}$,  its type $\mathsf{A}$ must be inferable,

\begin{code}\>\<%
\\
\>\AgdaFunction{id} \AgdaSymbol{:} \AgdaSymbol{\{}\AgdaBound{A} \AgdaSymbol{:} \AgdaPrimitiveType{Set}\AgdaSymbol{\}} \AgdaSymbol{→} \AgdaBound{A} \AgdaSymbol{→} \AgdaBound{A}\<%
\\
\>\AgdaFunction{id} \AgdaBound{a} \AgdaSymbol{=} \AgdaBound{a}\<%
\end{code}

The underscores are wildcards which can be automatically inferred or not used in the program construction. See the code above \ref{symmetry}.


\item \textit{Module system}. The mechanism of parametrised modules makes it possible to define generic operations and prove a whole set of generic properties.


\item \textit{Coinduction}. We can define coinductive types like streams in Agda which are typically infinite data structures. Coinductive occurences must be labelled with $\infty$ and coninductive types do not need to terminate but has to be productive. It is often used in conjunction with lazy evaluation. \cite{wiki:Coinduction}

For coinductive types and more generally mixed inductive/coinductive types \cite{txa:mpc2010g}, we adopt a set of operators which are defined in module \textbf{Coinduction}. A infinite list (or stream) can be defined as:

\begin{code}
\>\AgdaKeyword{data} \AgdaDatatype{Stream} \AgdaSymbol{(}\AgdaBound{A} \AgdaSymbol{:} \AgdaPrimitiveType{Set}\AgdaSymbol{)} \AgdaSymbol{:} \AgdaPrimitiveType{Set} \AgdaKeyword{where}\<%
\\
\>[0]\AgdaIndent{2}{}\<[2]%
\>[2]\AgdaInductiveConstructor{\_∷\_} \AgdaSymbol{:} \AgdaBound{A} \AgdaSymbol{→} \AgdaDatatype{∞} \AgdaSymbol{(}\AgdaDatatype{Stream} \AgdaBound{A}\AgdaSymbol{)} \AgdaSymbol{→} \AgdaDatatype{Stream} \AgdaBound{A}\<%
\end{code}

The delay operator $\infty$ denotes an coinductive argument. Given $a:A$, the expression with a delay function $\sharp~a$ is a element of type $\infty~A$. $\beta~x$ will force computation in $x : \infty~A$.
	
\end{itemize} 

Compared to Coq, it has no tactics including automated proving although it has ring solver which is similar to the tactic \textit{ring}. It is friendly to not only functional programmers but also people who are familiar with constructive Mathematics.

The syntax of Agda has some similarities to Haskell or \mltt, but there are some important differences,

\begin{itemize}
\item $\AgdaSymbol{:}$ is used for typing judgement, for example $\AgdaFunction{a} \AgdaSymbol{:} \AgdaDatatype{A}$, while double colons $\AgdaSymbol{::}$ is the \emph{cons} constructor for list.

\item "$\AgdaSymbol{=}$" is reserved for function definition as the conventions in programming languages. We use cogruence symbol $\AgdaDatatype{≡}$ for identity type. This is inconsistent with our conventional choices of symbols in articles.

\item The universe of small types is $\Set_{0}$ or $\Set$ instead of $\Type$, even though it is not a set in set-theoretical sense.
%We will follow the \textbf{typical ambiguity} in this thesis which says that we write $\AgdaBound{A} \AgdaSymbol{:} \AgdaPrimitiveType{Set}$ for $\AgdaBound{A} \AgdaSymbol{:} \AgdaPrimitiveType{Set} \AgdaBound{a}$ and $\AgdaPrimitiveType{Set} \AgdaSymbol{:} \AgdaPrimitiveType{Set}$ which stands for $\AgdaPrimitiveType{Set}\AgdaBound{i} \AgdaSymbol{:} \AgdaPrimitiveType{Set}\AgdaBound{(i+1)}$.

The universe of propositions $\Prop$ ($\Prop \subset \Set$) is not available. Therefore, a proposition is also in the universe $\Set$. Only necessary, we will axiomitise the proof-irrelevance property for a given proposition $P : \Set$.

\item Agda has a more liberal way to define $\Pi$-types. They are often written as special cases of function types, for example $\Pi x : A. B$ can be written as $(x : A) \to B$. $\Sigma$-types are defined in Agda standard library. There is also a generalised $\Sigma$-types called \emph{dependent record type} which can be defined by keyword \textbf{record}.

\item In Agda, we use Paulin-Mohring style identity type,

\begin{code}
\\
\>\AgdaKeyword{data} \AgdaDatatype{\_≡\_} \AgdaSymbol{\{}\AgdaBound{A} \AgdaSymbol{:} \AgdaPrimitiveType{Set}\AgdaSymbol{\}} \AgdaSymbol{(}\AgdaBound{x} \AgdaSymbol{:} \AgdaBound{A}\AgdaSymbol{)} \AgdaSymbol{:} \AgdaBound{A} \AgdaSymbol{→} \AgdaPrimitiveType{Set} \AgdaKeyword{where}\<%
\\
\>[0]\AgdaIndent{2}{}\<[2]%
\>[2]\AgdaInductiveConstructor{refl} \AgdaSymbol{:} \AgdaBound{x} \AgdaDatatype{≡} \AgdaBound{x}\<%
\\
\end{code}
It is parameterized by the left side of the identity which is equivalent to the original version.



\end{itemize}



\section{Extensional concepts}
\label{extensionality}

Objects are extensional equal if they have the same external properties for example point-wise equal functions, different proofs of the same proposition. They may not be intensionally equal but they can not be distinguished by any other terms. In \itt the extensionl equality is not captured by identity type and some other relevant extensional concepts are also not available. 

\textbf{Functional extensionality} 

In regular mathematics, the essence of functions is relations between inputs. In type theory functions can be defined in different ways. If two functions pointwise generate the same outputs for the same inputs, they are extensionally equal even though they may differ intensionally. This is called
functional extensionality which is not inhabited \cite{alti:lics99} in original
\itt and can be expressed as following,

\infrule[functional extensionality]{\Gamma \vdash f, g : A \to B \andalso \Gamma, a : A \vdash p: f(a) = g(a)}{\Gamma \vdash \text{ext}(p): f = g}



given two types $A$ and $B$, and two functions $f,\,g\,\colon A \to B$,

\[Ext = \forall\, x\colon A, f x = g x \to f = g\]


It is provable from equality reflection rule \ref{reflection},
 
\begin{proof}
Suppose $\Gamma \vdash f \,a = g \,a$, with reflection rule we have $\Gamma \vdash f \,a \equiv g \,a$.
Then using $\xi$-rule, we know that $\Gamma \vdash \lambda a . f \,a \equiv \lambda a . g \,a$.
From $\eta$-equivalence, we know that $\Gamma \vdash f \equiv g$. We can conclude that $\Gamma \vdash f = g$.
\end{proof}

In \itt, if we axiomatize it, the type theory is not adequate, which means they canonicity is lost.

\begin{proof}[Non-canonical construction]
Suppose we define $id \defeq \lambda x . x$ and $id' \defeq \lambda x . x + 0$, recursively it is provable that $p': \forall x , id~ x = id'~x$, from functional extensionality we obtain $\text{ext} ~p' : id = id'$. We can use substitution to construct non-canonical natural numbers, for instance $\text{subst}~ (\lambda f. \N) ~(\text{ext} ~p') ~0$. This is non-canonical because the equality is non-canonical (not refl) so that the expression cannot be normalised.
With this term, we can construct irreducible terms of any type $A$ by a
mapping $f : \N \to A$.
\end{proof}

This will destroy some good features of \itt. The non-canonical terms makes definitional equality undecidable and so is type checking. The termination of type checker is not assured and programs may be non-terminating.



\textbf{Uniqueness of Identity Proof (UIP)}

In \itt, the principle \emph{uniqueness of identity proofs}(UIP)

\begin{definition}\label{UIP}
Any two terms of a given identity type $p,q : x =_{A} y$ are themselves propositionally equal $p =_{x = y} q$.
\end{definition}

is not provable. It is equivalent to Streicher’s “Axiom K”

\begin{axiom}[K]
For all $x:A$ and $p: x=x$ we have $p=\text{refl}_{x}$.

From the usual definition of identity types, UIP is not a result for every type. UIP is also 
\end{axiom}

which has been shown not derivable from J by Hofmann and Streicher. 

However Agda's dependent pattern matching allows us to define UIP and K.
As we mentioned before, we would like to interpret types as groupoids or higher groupoids instead of sets, UIP does not hold in general. A type $A$ is a \textbf{set} (or h-set in \hott) if UIP holds for it.
Agda has a flag ``--without-K'' to restrict pattern matching.

Another characterisation of a set in type theory is given by Hedberg's Theorem.
\begin{theorem}[Hedberg]
If $A$ has decidable equality, then $A$ is a set.
\end{theorem}


There are also some relevant extensional concepts.

\textbf{Proof-irrelevance} 

All proofs of the same proposition are propositionally equal. In \itt since there is no universe for proposition, we usually write $P : \Set$ for a proposition which does not automatically gives us a proof that $\forall p, q : P, p = q$, hence there is no proof-irrelevance. If we have proof irrelevance, we can simply define identity type for sets as $x = y : \Prop$ and UIP is provable.

\textbf{Propositional extensionality} 

Two logically equivalent propositions are propositionally equal.

$$\forall P, Q : \Prop~ (P \iff Q) \to P = Q$$

\textbf{Quotient types} 

A quotient type is a type formed by redefing equality on a underlying type with a given equivalence relation on it. It is the main topic of this thesis and will be discussed in detail in next chapter.

\textbf{Univalence axiom}

Univalence is a extensional principle from homotopy theory which is an axiom in \hott. 
It states:

$$\forall A, B : \Type ~ (A \simeq B) \simeq A = B$$

Equivalence (a refinement of isomorphism) is equivalent to propositional equality.



We would like to have these extensional principles in \itt so that we have more tools to build both Mamthematics and programs. It is also valid and justifiable to extend Type Theory with them. However, it only makes sense if the type-checking decidability and terms canonicity are not sacrificed. 







% intuitionism
% There is another question, whether mathematics is a collection of patterns and laws which is observed, or it is a system created and built by people to explain the patterns and laws in the world. I think people prefer the second answer usually accept the type theory more easily, although most people (probably 99.9 percent) prefer the first one. When we learn what is natural numbers, we learn it as "numbers like 1, 2, 3 ,4 and perhaps 0", the commutative law, associate law are axioms because there is no way to prove it if we introduce it in this manner. We are convinced by some examples like "2 + 3 = 3 + 2" and we find it works for most of the cases then we accept it by observations. It is some methods physicians used a lot -- to conclude some laws from a number of facts. It is a proper method for physicians because what they research on is world can only be observed. However for mathematics, even though it is applied to the real world, it is a system completely created by people. People used their fingers to count, wrote symbols for results, even though it was very shallow it is obviously a aritificial system. People extend 


% Type theory is strongly connected with computation theory.
% Set

% Type theory has fewer axioms, simpler model than set theory which has mutual foudations: logic and axioms.




%although the absence of \emph{principle of excluded middle} in intuitionistic logic makes some mathematicians hard to accept. There is a very good talk given by Andrej Bauer in IAS called "Five Stages of Accepting Constructive Mathematics'' online \footnote{available on Youtube}.



%is worth more studying. It is more close to program construction and from a computer scientist's point of view, it is very natural to accept intuitionistic logic. 


\section{Homotopy Type Theory}

\hott is a new field developed between theoretical computer science
and Mathematics.
It arises from a new interpretation of intensional \mltt
into homotopy theory where identity types are interpreted as paths in homotopy theory.
It accepts Vladimir Voevodsky's \textbf{univalence axiom} which identifies isomorphic
structures. The axiom implies many extensional concepts, for example functional extensionality and also quotient types. In fact it has a stronger notion which is \textbf{higher inductive types}.


\subsection{Homotopic interpretation}

Types are usually interpreted as sets in \mltt, but the identity type of types enforces a more sophisticated structure on types compared to the the one on sets due to the missing Axiom $\mathsf{K}$ for which asserts that all inhabitants equal to the only constructor $\mathsf{refl}$. 

Inspired by the groupoid model of (intensional) Martin-Löf type theory due to Hofmann and Streicher, Awodey, Warren \cite{awodey-warren} and Voevodsky \cite{VV} develops \hott which is a homotopic interpretation of \mltt.

In \hott, types are regarded as spaces (or higher groupoids) instead of sets, terms are "points" of types. A function $f : A \to B$ is a continous map between spaces $A$ and $B$.

\begin{itemize}
\item Types are interpreted as spaces. $a : A$ can be stated as $a$ is
  a point of space $A$.
\item Terms are continous functions, for example, $f : A \rightarrow B$ is a
  continous function between spaces and it is equivalent to say $a$ is
  a point of space or $a : 1 \rightarrow A$ is a continous function.
\item Identity types are path spaces.
\item Identity types of identity types are homotopies (if a path is considered as a continous function $p : [0,1] \rightarrow X$).
\item There are also 3-homotopies and 4-homotopies and higher
  levels which forms an infinite structure called \og (or $\infty$-groupoids) in higher category theories.
\end{itemize}

\begin{remark}
It has to be emphasised that the notions like spaces are purely homotopically, in other words, there is no topological notions like open sets in \hott. Therefore it is more appropriate to interpret types as \og instead of spaces. 

The notion of \og can be seen as a generalisation of groupoid which has infinite levels of "isomorphisms" corresponding to the infinite tower of iterated identity types, i.e.\ the identity type of identity type, the identity type of identity type of identity type ... 


\begin{definition}
\textbf{\ogs}.

An $\omega$-groupoid is an $\omega$-category where all $k$-morphisms for all $k \in \N$ are equivalences. 
\end{definition}

\begin{definition}
\textbf{Equivalence}.

In higher category theory an equivalence is a morphism which is invertible up to all higher equivalences.
\end{definition}


The notion of equivalence can be seen as a refinement of isomorphism without UIP \cite{txa:csl}. 
In the higher-categorical setting, equivalence can be thought of as arising from isomorphisms by systematically replacing equalities by higher cells.
For example, an equivalence 
between two objects $A$ and $B$ in a 2-category is a morphism $f : A \rightarrow B$ which has a
corresponding inverse morphism $ g : B \rightarrow A$, but instead of the
equalities $f ∘ g = 1_B$ and $g ∘ f = 1_A$ we have 2-cell isomorphisms $f ∘ g ≅ 1_B$ and $g ∘ f ≅ 1_A$. In an $\omega$-category, these later isomorphisms are equivalences again.
These equivalence is weak in the sense that they hold up to all higher equivalences. 
As all equivalences here are weak equivalences, from now on we say just equivalence.
In fact the \og used to model the interated identity types is also weak, so precisely we should call them \wog.

\end{remark}



To distinguish these structured objects interpretation with usual set-like types, we also call them \textbf{homotopy types}. Although we interpret all types as \og, most types behaves internally like a $k$-groupoid, which is called trucation.

\begin{definition}
\textbf{Trucated $n$-groupoid.}
An $n$-truncated \ogs is an $n$-groupoid.
\end{definition}

In \hott the notion of \textbf{homotopy $n$-types} are analogous to $n$-groupoids in higher category theory,
A set can be seen as a discrete space which is $0$-groupoid. Thus a set is called homotopy $0$-type or h-set which is of \textbf{homotopy level} $2$. It is a fact that the identity type of an $n+1$-type is an $n$-type, for example, the identity type of a groupoid is a set. Following this relation, a $-1$-type is a proposition (mere proposition or h-proposition in \hott) and $-2$-type is a unit type or contractible type.

\textbf{Univalence Axiom}

Voevodsky recognised that the homotopic interpretation is \emph{univalent} which means isomorphic types are equal, which does not usually hold in \itt. 

\begin{definition}
\textbf{Univalence axiom}. for any two types $X$ and $Y$, the function 
$$f : X = Y \to X \simeq Y$$ is an equivalence.
\end{definition}

It is one of the fundamental axioms of the \hott and is central to the Voevodsky's proposal of Univalent Foundation Project \cite{vv_uf}. 
It can be viewed as a strong extensionality which does imply functional extensionality (a coq proof of this can be found in \cite{uafe}). 

Since isomorphic types are cosidered the same, all constructions and proofs are automatically transported between them, and it actually makes reasoning abstract.


\textbf{Higher inductive types}

In \itt, types are treated as sets and we use \emph{inductive types} \ref{df:inductivetypes} to define sets which have only "points". However, in \hott, inductive types are not enough in expresiveness, because types have higher structures namely the interated identity types on each levels.
A more general schema to define types including higher paths is required which is \textbf{higher inductive types}. 

\begin{definition}
\textbf{Higher inductive types} (HITs).
Higher inductive types allow constructors for not only points of the type being defined, but also elements of its iterated identity types.
\end{definition}

It is also one of the fundamental axioms of \hott.
One commonly used example is a circle
$\base:\Sn^1$ (1-sphere) can be \emph{inductively} defined as:

\begin{itemize}
\item A point $\base:\Sn^1$, and
\item A path $\lloop : {\id[\Sn^1]\base\base}$.
\end{itemize}

It is also essential to provide the elimination rule for the paths as well. Categorically speaking, it means the functions have to be functorial. For example, to define a function $f :\Sn^1 \rightarrow B$, assume $f(base)=b$, we have to map $\lloop$ to an identity path $l : b = b$, i.e.\ $ap_{f}(\lloop)=l$.


For further explanation of \hott, a well-written text book elaborated by many brilliant mathematicians and computer scientists is available online \cite{hott}. 



\subsection{Extensionality in \hott}

In \hott, many extensional concepts are automatically derivable. As we have mentioned, functional extensionality is implied by univalence, propositional extensionality is just an instance of univalence, moreover quotient types are indeed a specific way of applying higher inductive types which are more powerful.

In the usual \itt, types are sets so the quotient types are simply adding level-$1$ equivalence in the structure of given sets. 
In this sense, each non-trivial morphism derived from an equivalence relation is the unique
one. This looks like a setoid, but indeed there is no difference
between sets and setoids categorically, because a skeleton of a setoid
is just a discrete category which is equivalent to a set.


Assume $A$ is a set (h-set) and $\_\sim\_ : A \to A \to \Prop$
\footnote{$\Prop$ stands for mere propositions in \hott} is an
equivalence relation. The quotient set $\qset{A}$
can be defined as a higher inductive type:

\begin{itemize}
\item $[\_] : A \rightarrow \qset{A}$
\item $eqv : (a,b : A) \rightarrow A \sim B \rightarrow  [a] = [b]$

the resulting type is a set as well, so

\item $isSet : (x,y:\qset{A}) \rightarrow (p_1,p_2 : x = y) \rightarrow p_1 = p_2$

\end{itemize}


Note that it is not a generic quotient construction of any types but specially for sets, so
we also call it \textbf{set quotient} or QITs (Quotient inductive types). In fact, since types are essentially structured objects, it is possible to quotient any kinds of types with any kinds of equivalences which can be not propositional.
 
\subsection{QITs v.s. Quotient types}

We can prove that QITs are more powerful than quotient types using an
example called unordered trees (rooted tree). An unordered tree is a tree connected to a multiset of rooted trees, hence there is no ordering on subtrees.

Firsrtly we define ordered trees as:

\begin{itemize}
\item A leaf $l: \mathsf{Tree}$, or
\item An ordered list of subtrees indexed by $\mathbb{N}$, $st : (\mathbb{N} \rightarrow \mathsf{Tree}) \rightarrow \mathsf{Tree}$,
\end{itemize}

With the following equivalence relation:

\begin{itemize}
\item $l_{eq} : l \sim l$,
\item $st_{eq} : (f , g : \N \to \mathsf{Tree}) \to f \sim_{p} g \to sp~f \sim sp~g$
\end{itemize}

where $f \sim_{p} g$ stands for $f$ is a permutation of $g$. The permutation can be defined using a bijective map $p : \N \to \N$ which relates equivalent subtrees recursively.

%However, if we use quotient type $\mathsf{Tree^{\sim}} := \qset{\mathsf{Tree}}$, the resulting trees have unordered subtrees which are themselves ordered.

However, the equivalence relation can be undecidable which means that if we use quotient type $\mathsf{Tree^{\sim}} := \qset{\mathsf{Tree}}$, it is impossible to lift constructor $\overline{st} : (\N \to \mathsf{Tree}) \to \mathsf{Tree}$. 
We use a symbol $\mathsf{Tree}^{\sim}$ for unordered trees.

For finitary branching trees like binary trees whose equivalence relation is decidable, $(\textbf{2} \rightarrow \mathsf{BTree}^{\sim}) \to \mathsf{BTree}^{\sim}$ is ismorphic to $\mathsf{BTree}^{\sim} \rightarrow \mathsf{BTree}^{\sim} \rightarrow \mathsf{BTree}^{\sim}$ the lifting of functions can be nested,

$\overline{st}~a~b = \text{lift}~(\text{lift}~st~a)~b$

Intuitively it is impossible to apply this approach to $\mathsf{Tree}$ because we cannot eliminate infinite number of $\mathsf{Tree^{\sim}}$.


%$\overline{st} : (\mathbb{N} \rightarrow \mathsf{Tree^{\sim}}) \rightarrow \mathsf{Tree^{\sim}}$

%such that for any $f : \mathbb{N} \rightarrow \mathsf{Tree}$

%$\overline{st}~ ([\_] \circ f) = [ st~f ] $



\todo{not so clear why}
It means that we are unable to define unordered tree using
quotient types because the constructors and equivalence have to be
defined mutually.



%To define a function on permutation trees, it is necessary to define them as

%$f : Tree~A \rightarrow B$ which respects the equivalence relation $a~b : Tree~A, f ~a = f ~b$




Higher inductive types, specially quotient inductive types, allow  us to define constructors of trees and paths simultaneously.

\begin{itemize}
\item $l: \mathsf{Tree}$, 
\item $st : (\mathbb{N} \rightarrow \mathsf{Tree}) \rightarrow \mathsf{Tree}$,
  and
\item a set of pathes relates two permutated trees:

$l_{eq} : l  =_{\mathsf{Tree}} l $

$st_{eq} : \forall (f, g : \N \to \mathsf{Tree}) \rightarrow
f \sim_{p} g \rightarrow  st~f =_{\mathsf{Tree}} st~g$
\end{itemize}


%\subsection{Quotient inductive inductive types}


\section{Summary}


The theory of types was originally invented to solve inconsistence in set theory in 1900s. After that, mathematicians developed it by adding more properties, for example functions as primitive types, Curry-Howard isomorphism. Type theory is closely related to type systems in programming languages, and some type theories like the simply-typed lambda calculus, Per Martin L\"{o}f's Instuitionistic type theory (Type Theory) and the calculus of constructions, are themselves programming languages. 

Martin L\"{o}f's type theory is one of the most modern type theories which is closely related to constructive Mathematics and computer science. It is a formal system given by a sequence of rules written as derivations of judgements. Because of Curry-Howard isomorphism and dependent types, we can implement intuitionistic logic in Type Theory. It means that we can do constructive reasoning by program constructions. From a mathematician's point of view, it provides computer aid formal reasoning in languages like Agda and Epigram. From a a programmer's point of view, it provides program verifications in itself and a more expressive to write specification of programs.

The intensional version of \mltt has decidable type checking which is a more popular choice in programming languages. Agda is one of these languages and it has many good features supporting mathematical constructions and reasoning. It is used a lot in academia by theoretical computer scientist and mathematians for example the \hott community. We will also do our research work in it.
Despite the good properties of \itt, it lacks many extensional concepts like functional extensionality and quotient types. Many researches have been done to add them into type theory wihtout losing good features. This thesis is one of the attempt in this direction.



