\chapter{Type Theory}
\label{bg}

Type theory is usually used to refer to a formal systems in which every term always has its type. It is initially invented as a foundation for mathematics as an alternative to set theory, and later works well in computer science as programming languages which we can write certified programs with type system. There are various type theories, but here we mainly study Per Martin-L\"{o}f's intuitionistic type theory (N.B.\ we will use "type theory'' specially for it if not ambiguous). It is one of the most widely studied type theory in computer science and has been implemented as programming languages like Agda, Coq, NuPRL, Epigram etc. Another type theory will play important role in this thesis is Homotopy Type Theory which is a comparatively new field between mathematics and computer science which interprets type theory using notions in homotopy type theory.

In this chapter we will introduce type theory, especially \itt, from history to rules. We will also introduce one of the implementation of intensional type theory -- Agda briefly. We will also discuss the topic of extensional concepts which are missing from \itt as a prerequisite for the topic of our thesis -- quotient types.



\section{From set theory to type theory}

The concept of \emph{sets} has been used since thounsands of years ago. However mathematician have not study the theory of sets until 1870s when George Cantor and Richard Dedekind founded set theory as a branch of mathematical logic. Since then set theory is used as a language to describe definitions of most mathematical objects, namely it is a foundational system for mathematics.



However, in the 1900s, Bertrand Russell discovered a paradox in their system. In this naive set theory, there was no distinction between small sets like the set of natural numbers or the set of real numbers and "larger" sets like the set of all sets. This lead to Russell's paradox.

\begin{example}[Russell's Paradox]
Let $R$ be the set of all sets which do not contain themselves
$R = \{x ~| ~x \not\in  x\}$
Then we got a cotradiction
$R \in R \iff R \not\in R$
\end{example}

To avoid this paradox, Russell
proposed the theory of types \cite{rus:1903} as an alternative to
naïve set theory. Each mathematical object is assigned a type. This is done in a hierarchical structure such that "larger" sets and small sets reside in different levels. The set of all
sets is no longer on the same level as its elements and the
paradox disappears.

The elementary notion of type theory is \emph{type} which plays a similar role to set in set theory, but differs fundamentally. Every object in type theory comes with its unique type, while an object in set theory can appear in multiple sets and we can talk about an object without knowing which set it belongs to.
To explain the difference, we use the the number $\mathsf{2}$ as an example. In set theory, 2 is not an element of only one specific set, it belongs to the set of natural numbers $\N$ and also the set of integers $\Z$. While in type
theory, it is impossible to avoid mentioning the type of object $\mathsf{2}$. Usually the term $\AgdaInductiveConstructor{suc} \, \AgdaSymbol{(}\AgdaInductiveConstructor{suc} \, \AgdaInductiveConstructor{zero}\AgdaSymbol{)}$ stands for $\mathsf{2}$ of type $\N$ and we have a different term of type $\Z$ constructed by constructors of $\Z$ which is a different object to the one of $\N$. 

Since Russell's type theory, a variety of type theories have been developed by mathematicians and computer scientists, for example Gödel's System T \cite{gdl:1931}. There are two families of famous type theories building the bridges between mathematics and computer science, \emph{lambda calculus} and \emph{\mltt}.


\subsection{Lambda Calculus}

Alonzo Church introduced lambda calculus in the 1930s. He first introduced
an untyped lambda calculus which turned out to be inconsistent due to
the Kleene-Rosser paradox\cite{kleene1935inconsistency}.

\begin{example}[Kleene-Rosser paradox]
Suppose we have a function $f = \lambda x . \neg (x ~ x) $, then we can deduce a contradiction by applying it to itself:

$f f = (\lambda x . \neg (x ~ x)) f = \neg (f ~ f)$
\end{example}

Then he refined it with adding types. This theory is also called
Church's theory of types or simply typed lambda calculus is
introduced as a foundation of mathematics. An important change is that functions become primitive objects which means functions are types defined inductively using the $\rightarrow$ type former. It is widely applied to
various fields especially computer science. Some languages are extentions of lambda calculus, for example Haskell. Haskell belongs to one of the variants of lambda calculus called System F, although it has evolved into System FC recently. There are also other refinements of lambda calculus which is illustrated by the $\lambda$-cube \cite{barendregt1991introduction}.



\subsection{Per Martin-L\"{o}f's Type Theory}

In 1970s, Per Martin-L\"{o}f \cite{per:71,per:82}  developed his profound intuitionistic type theory. His 1971's formulation which is impredicative was proved to be inconsistent because of Girard's paradox \cite{hurkens1995simplification}. The impredicativty means that for a universe $\mathsf{U}$, there is an axiom $\mathsf{U} \in \mathsf{U}$. The later version is predicative and is more widely used.


It serves as a foundation of constructive mathematics \cite{martin1984intuitionistic}. Different to set theory whose axioms are based on first-order logic, \mltt provides a means of implementing intuitionistic logic. This is achieved by the Curry-Howard
isomorphism\emph{``propositions can be interpreted as types and their
  proofs are inhabitants of these types''}. It relates computer programs with mathematical proofs. Technically, a proposition can be encoded as a type, and then a proof of it can be given by constructing a program (or term) of it. It internalises the  Brouwer–Heyting–Kolmogorov (BHK) interpretation of intuitionistic logic, for example a proof of $P \wedge Q$ is a pair of $p : P$ and $q : Q$.
Started from these, implications are functions, negations are functions into empty types, modus ponens is function application etc. This makes it a programming makes it a programming languages in which we can certify programs within the same language.
Compared to set theory, type theory has less axioms and gives us the possiblity of construct mathematics in computers so that we can do theorem proving and verification with the help of computers.

Another important feature of \mltt is dependent types. The type systems of simply lambda calculus like Haskell are not expressive enough to encode predicate logic without dependent types.

\begin{definition}\label{dpty}
\textit{Dependent type}. Dependent types are types that depends on values of other types \cite{dtw}. 
\end{definition}

As a example, it is possible to write a type for a list of natural number of length 3 as $List~\N ~3$. 

With dependent types, the quantifiers like $\forall$ and $\exists$ can be encoded.
The Curry-Howard isomorphism is then extended to predicate logic. 
A predicate on $X$ can be written as a dependent type $P ~x$ where $x : X$. 


\textbf{Equalities in Type Theory}
The notion of equality is one of the most profound topic in type theory.
we have two kinds of equality, one is definitional equality, the other is propositional equality.

\begin{definition}
\textit{Definitional equality} is a judgement-level equality, which holds when two objects have the same normal forms\cite{nor:90}.
\end{definition}

% Objects are definitional equal if they normalise to the same form. 
% Usually types like $a \equiv b$ stands for a definitional
% equality between $a$ and $b$. It is some primitive jundgements which are part of the
% meta-theory rather than construction like types or terms . Definitional
% equality can be judged and decided by type-checker. 


With dependent types, it is possible to write a type to encode the equality of objects.

\begin{definition}
\textit{Propositional equality} is a type which represents a propostion that two objects of the same type are equal.
\end{definition}

Intuitively if two objects are definitionally equal, they must be propositionally equal.

\begin{equation*}
\infer[\text{Id-intro}]{a = b}{a \equiv b}
\end{equation*}

But how about the other way around (we call it the \emph{equality reflecition rule})? Are two propositional equal objects definitional equal?
\begin{equation}
\label{reflection}
\infer[\text{Reflection}]{a \equiv b}{a = b}
\end{equation}
 Actually
the treatment of equality creates two different versions of \mltt, \emph{intensional} one and \emph{extensional} one.

In \itt, the answer is no. Propositional equality (also called intensional equality  \cite{nor:90}) is different to definitional equality. 
The definitional equality is always decidable hence type checking that depends on definitional equality is
decidable as well~\cite{alti:lics99}. Therefore \itt has better computational behaviors.
Types like $\mathsf{a = b}$ which stands for a
propostion that $\mathsf{a}$ equals $\mathsf{b}$ are propositional equalities. They are some types which we need to prove
or disprove by construction. Each of them has an unique element $\mathsf{refl}$ which only exists if $\mathsf{a}$ and $\mathsf{b}$ are
definitionally equal in all cases. However it is not enough for other extensional equalities, for example the equality of functions.


\ett adopted the equality reflection rule which means the propositional equality is extensional and is undistinguished with definitional equality, in other words, two propositional equal objects are judgementally equal.

There are some extensional concepts exists in \ett. For example the functional extensionality.

\begin{definition}[Functional extensionality]\label{fun-ext}
Objects of different normal forms, for example point-wise equal functions or different proofs of the same proposition, may be definitionally equal. This is called functional extensionality.

\begin{equation}
\infer[\text{fun-ext}]{f = g}{f g : A \rightarrow B, \forall a : A, f \, a = g \, a}
\end{equation}

\end{definition}



\begin{lemma}
\label{functional extensionality is available in ett}
Functional extensionality \ref{fun-ext} is derivable from equality refleciton rule \ref{reflection} (in \ett).
\end{lemma}
 
\begin{proof}
Suppose $\Gamma \vdash f \,a = g \,a$, with reflection rule we have $\Gamma \vdash f \,a \equiv g \,a$.
Then using $\xi$-rule, we know that $\Gamma \vdash \lambda a . f \,a \equiv \lambda a . g \,a$.
From $\eta$-equivalence, we know that $\Gamma \vdash f \equiv g$. We can conclude that $\Gamma \vdash f = g$.
\end{proof}

In \itt, this is not provable. If we add it as an axiom, we will lose the canonicity since we can construct a natural number with this extensionality and substitution for propositional equality.

\begin{example}[non-canonical construction]
Suppose we define $id := \lambda x . x$ and $p0 := \lambda x . x + 0$, recursively it is provable that $p': \forall x , id~ x = p0~x$, from functional extensionality we obtain a propositional equality $p: id = p0$ such that $p \equiv ext ~p'$. We can use substitution to construct non-canonical natural numbers, for instance $subst~ (\lambda f. \N) ~p ~0$. This is non-canonical because the equality is non-canonical so that the expression cannot be normalised.
\end{example}


The non-canonicity makes the definitional equality undecidable and so is type checking. The termination of type checker is not assured.


The choice whether adopt equality reflection rule or not decides the version of \mltt. Agda chooses the intensional one while NuPRL chooses the extensional one.  However we consider intensional version more as a better choice because it ihas better computational behaviors. Even though we do not get the extensional concepts automatically, we still want them in \itt and people are keeping on studying the way to introduce these into \itt.

Altenkirch and McBride introduced a variant of \ett called
\emph{Observational Type Theory}  \cite{alt:06} in which definitional equality is
decidable and propositional equality is extensional.


We already know that \mltt can be encoded as programming languages in
which the evaluation of a well-typed program always terminates \cite{nor:90}.


\section{Agda}

Agda is a dependently typed functional programming language which is designed based on intensional version
of \mltt \cite{agdawiki:main}. It is used as the main tool to study the topic of this thesis.

As we have seen, \mltt is based on the Curry-Howard
isomorphism: types are identified with propositions and terms (or programs) are identified with proofs. It turns Agda is into a proof assistant like Coq, which allows users to do mathematical reasoning and also computer program reasoning. 
Usually to prove the correctness of programs, we need to state some theorems of programming languages on the meta-level, but in Agda we can prove and use these theorems alongwith writing programs.  As Nordström et al. \cite{nps} pointed out that we could express both specifications and programs at the same time when using the type theory to construct proofs using programs.


\todo{following paragraph needs to be rewritten or added with code}
We can prove a proposition following steps below:
First we give the name of the proposition and encode it as the type. Then we can gradually refine the goal to formalise a type-correct program namely the proof. As long as we have the proof, it can be used as a lemma in other proofs or programs. Usually, there are no tactics like in Coq (it may be implemented in the future). But with the gradually refinement mechanism, the process of building proofs is very similar to conceiving proofs in regular mathematics.


There are more features of Agda as follows:

\begin{itemize}
\item \textit{Dependent type}. 
As mentioned in \ref{dpty}, dependent types are types that depends on values of other types \cite{dtw}. They enable us to write more expressive types as program speficication or propositions in order to reduce bugs. In Haskell and other Hindley-Milner style languages, types and values are clearly distinct \cite{tutorial}, In Agda, we can define types depending on values which means the border between types and values is vague. To illustrate what this means, the most common example is $\mathsf{Vector A n}$ where we can length-explicit lists called vectors. It is a data type which represents a vector containing elements of type \textbf{A} and depends on a natural number \textbf{n} which is the length of the list. We can specify types with more constraints such that the we can express what programs we can better and leave the checking work to the type chekcer. For instance, to use the length-explicit vector, we will not encounter exceptions like out of bounds in Java, since it is impossible to define such functions before compiling.

\item \textit{Functional programming language}. As the name indicates that, functional programming languages emphasizes the application of functions rather than changing data in the imperative style like C{}\verb!++! and Java. The base of functional programming is lambda calculus. The key motivation to develop functional programmming language is to eliminating the side effects which means we can ensure the result will be the same no matter how many times we input the same data. There are several generations of functional programming languages, for example Lisp, Erlang, Haskell etc. Most of the applications of them are currently in the academic fields, however as the functional programming developed, more applications will be explored.

% \item \textit{Per Martin-Löf Type Theory}. It has different names like Intuitionistic type theory or Constructive type theory and is developed by Per Martin-Löf in 1980s. It associated functional programs with proofs of mathematical propositions written as dependent types. That means we can now represent propositions we want to prove as types in Agda by dependent types and Curry-Howard isomorphism \cite{aboa}. Then we only need to construct a program of the corresponding type to prove that propostion. For example:

\end{itemize}

As a functional programming languages, Agda also has some nice features for theorem proving,

\begin{itemize}

\item \textit{Pattern matching}. The mechanism for dependently typed pattern matching is very powerful \cite{alti:pisigma-new}. We could prove propositions case by case. In fact it is similar to the approach to prove propositions case by case in regular mathematics. Pattern match is a more intuitive way to use the eliminators of types. For example, to define the negation on booleans $neg : Bool \to Bool$, we can write the program in two cases with respect to the value of the argument.

\item \textit{Inductive \& Recursive definition}. In Agda, types are often defined inductively, for example, natural numbers is defined as

\begin{code}\>\<%
\>\AgdaKeyword{data} \AgdaDatatype{ℕ} \AgdaSymbol{:} \AgdaPrimitiveType{Set} \AgdaKeyword{where}\<%
\\
\>[0]\AgdaIndent{2}{}\<[2]%
\>[2]\AgdaInductiveConstructor{zero} \AgdaSymbol{:} \AgdaDatatype{ℕ}\<%
\\
\>[0]\AgdaIndent{2}{}\<[2]%
\>[2]\AgdaInductiveConstructor{suc} \<[7]%
\>[7]\AgdaSymbol{:} \AgdaSymbol{(}\AgdaBound{n} \AgdaSymbol{:} \AgdaDatatype{ℕ}\AgdaSymbol{)} \AgdaSymbol{→} \AgdaDatatype{ℕ}\<%
\>\<\end{code}

The function for inductive types are usually written in recursive style, for example, the double function for natural numbers,

\begin{code}\>\<%
\\
\>\AgdaFunction{double} \AgdaSymbol{:} \AgdaDatatype{ℕ} \AgdaSymbol{→} \AgdaDatatype{ℕ}\<%
\\
\>\AgdaFunction{double} \AgdaInductiveConstructor{zero} \AgdaSymbol{=} \AgdaInductiveConstructor{zero}\<%
\\
\>\AgdaFunction{double} \AgdaSymbol{(}\AgdaInductiveConstructor{suc} \AgdaBound{n}\AgdaSymbol{)} \AgdaSymbol{=} \AgdaInductiveConstructor{suc} \AgdaSymbol{(}\AgdaInductiveConstructor{suc} \AgdaSymbol{(}\AgdaFunction{double} \AgdaBound{n}\AgdaSymbol{))}\<%
\>\<\end{code}

The availability of recursive definition enables programmers to prove propositions in the same manner of mathematical induction. 

\item \textit{Construction of functions}. One of the advantage of using a functional programming language as a theorem prover is the construction of functions which makes the proving more flexible.

In functional programming languages, complicated programs are commonly built gradually using aunxiliary functions and frequently used functions in the library.

Described as a proof assistant, complicated theorems are commonly proved gradually using lemmas and other theorems we have proved.

This decreases the difficulty of interpreting proofs in mathematics into Agda.

\item \textit{Lazy evaluation}. Lazy evaluation could eliminate unecessary operation because Agda is lazy to delay a computation until we need its result. It is often used to handle infinite data structures. \cite{wiki:Lazy_evaluation}

\end{itemize}

Agda also has some special functions in its interactive emacs interface beyond simple functional programming languages which enhance the ease and convenience of this language.

\begin{itemize}
\item \textit{Type Checker}. Type checker is an essential part of Agda. You can use to to type check a file without compiling it. It is the type checker that detect type mismatch problem and for theorem proving, it means the proof is incorrect. It interactively shows the goals, assumptions and variables when buiding a proof. 

The \emph{coverage checker} makes sure that the patterns cover all possible cases \cite{aboa}. 

The \emph{termination checker} will warn possiblily non-terminated error. The missing cases error will be reported by type checker. The suspected non-terminated definition can not be used by other ones. All programs must terminate in Agda so that it will not crash \cite{tutorial}.  The type checker then ensures that the proof is complete and not been proved by itself. 

In Agda, type signatures for functions are essential due to the presence of type checker (which is different to Haskell).
 
\item \textit{Interactive interface}. It has a Emacs-based interface for interactively writing and verifying proofs.  With type checker we can refine our proofs step by step \cite{aboa}. It also has some convenient functions and emacs means the potential to be extended.

\item \textit{Unicode support}. In Haskell and Coq, unicode support is not an essential part. However in Agda, to be a better theorem prover, it reads unicode symbols like: $\beta$, $\forall$ and $\exists$ and supports mixfix operators like: $+$ and $-$, which are very common for mathematics. It provides more meaningful names for types and lemmas and more flexible way to define operators. This also improve the readablity of the Agda proofs. For example, the commutativity of plus for natural numbers can be encoded as follows

\begin{code}
\>\AgdaFunction{comm} \AgdaSymbol{:} \AgdaSymbol{∀} \AgdaSymbol{(}\AgdaBound{a} \AgdaBound{b} \AgdaSymbol{:} \AgdaDatatype{ℕ}\AgdaSymbol{)} \AgdaSymbol{→} \AgdaBound{a} \AgdaFunction{+} \AgdaBound{b} \AgdaDatatype{≡} \AgdaBound{b} \AgdaFunction{+} \AgdaBound{a}\<%
\end{code}

We can use symbols we are familiar in regular mathematics.



Secondly we could use symbols to replace some common-used properties to simply the proofs a lot. The following code was simplied using several symbols,


Finally, we could use some other languages characters to define functions such as Chinese characters.

\item \textit{Code navigation}. As long as a program is loaded, it provides shortcut keys to move to the original definitions of certain object and move back. In real life programming it alleviates a great deal of work of programmers to look up the library.

\item \textit{Implicit arguments}. Sometime it is unnessary to write an argument since it can be inferred from other arguments by the type checker. It can simplify the application of functions and make the programs more concise. For example, to define a polymorphic function $\mathsf{id}$,

\begin{code}\>\<%
\\
\>\AgdaFunction{id} \AgdaSymbol{:} \AgdaSymbol{\{}\AgdaBound{A} \AgdaSymbol{:} \AgdaPrimitiveType{Set}\AgdaSymbol{\}} \AgdaSymbol{→} \AgdaBound{A} \AgdaSymbol{→} \AgdaBound{A}\<%
\\
\>\AgdaFunction{id} \AgdaBound{a} \AgdaSymbol{=} \AgdaBound{a}\<%
\end{code}

Whenever we give an argument $\mathsf{a}$,  its type $\mathsf{A}$ must be inferable.

\item \textit{Module system}. The mechanism of parametrised modules makes it possible to define generic operations and prove a whole set of generic properties.

\item \textit{Coinduction}. We can define coinductive types like streams in Agda which are typically infinite data structures. Coinductive occurences must be labelled with $\infty$ and coninductive types do not need to terminate but has to be productive. It is often used in conjunction with lazy evaluation. \cite{wiki:Coinduction}
	
\end{itemize} 

With these helpful features, Agda is a very powerful proof assisstant. It does not magically prove theorems for people, but it really helps mathematicians and computer scientists to do formalised reasoning with verification by high-performance computers. 


\subsection{Basic syntax}

Agda is a functional programming language and at the same time a theorem prover for mathematician. Its syntax has some similarities with Haskell but there are also many differences. In the other perspective, even though it is an implementation of \mltt, it adopts a syntax more closely to functional programming languages.

\begin{itemize}
\item Since Agda has a type checker, the programmer use typing judngement more often, so the designer decide to use single colon $\AgdaSymbol{:}$ for typing judgement, for example $\AgdaFunction{a} \AgdaSymbol{:} \AgdaDatatype{A}$ means that $\AgdaFunction{a}$ is of type $\AgdaDatatype{A}$, while double colons $::$ for the definition of the \emph{cons} constructor for list.

\item  The symbols for different equalities are contrary to the conventions in text. Like in some other programming languages e.g.\ Java Haskell, the equality symbol "$\AgdaSymbol{=}$" is reserved for function definition. Instead the cogruence symbol "$\AgdaDatatype{≡}$ is chosen for identity type which internalise propositional equality. 
% This is inconsistent with our conventional choices of symbols in articles, but it follows the conventions in Haskell and other programming languages that "$\AgdaSymbol{=}$" is used for definition.

\item Agda provides a more flexible way to define mixfix operators. With the unicode support, it is possible to define infix plus operator $\_+\_$, where the underscore marks the spaces for the explicit arguments in non-prefix operators. Underscores in expressions like $f ~ x ~ \_ z$ represent wildcards in function application for unnecessary  arguments (namely if can be inferred by type checker).

\item We use \textbf{data} to define constructors for inductive and coinductive datatypes. 

\item We have universe levels parameters in a lot of definitions which makes code looks unnecessarily cumbersome. 
The universe of small types is encoded as $\Set_{0}$ or $\Set$ rather than $\Type$, even though it is not a set in set-theoretical sense.
We will follow the \textbf{typical ambiguity} in this thesis which says that we write $\AgdaBound{A} \AgdaSymbol{:} \AgdaPrimitiveType{Set}$ for $\AgdaBound{A} \AgdaSymbol{:} \AgdaPrimitiveType{Set} \AgdaBound{a}$ and $\AgdaPrimitiveType{Set} \AgdaSymbol{:} \AgdaPrimitiveType{Set}$ which stands for $\AgdaPrimitiveType{Set}\AgdaBound{i} \AgdaSymbol{:} \AgdaPrimitiveType{Set}\AgdaBound{(i+1)}$.
The universe of propositions $\Prop$ ($\Prop \subset \Set$) does not exists in Agda because there is no proof-irrelevance in \itt as we will see later \ref{extensionality}. For ease of reading, we will use $\Set$ in replace of $\Prop$ in Agda code. Only necessary, we will explicitly add the proof-irrelevance property for a given proposition $P : \Set$, i.e.\ for all $p~ q : P$, $p = q$.


\item Agda has a more liberal way to define $\Pi$-types. They are often written as special cases of function types, for example $\Pi x : A. B$ can be written as $(x : A) \to B$. $\Sigma$-types are defined in Agda standard library. There is also a generalised $\Sigma$-types called \emph{dependent record type} which can be defined by keyword \textbf{record}.

\item For coinductive types and more generally mixed inductive/coinductive types \cite{txa:mpc2010g}, we adopt a set of operators which are defined in module \textbf{Coinduction}. A infinite list (or stream) can be defined as:

\begin{code}
\>\AgdaKeyword{data} \AgdaDatatype{Stream} \AgdaSymbol{(}\AgdaBound{A} \AgdaSymbol{:} \AgdaPrimitiveType{Set}\AgdaSymbol{)} \AgdaSymbol{:} \AgdaPrimitiveType{Set} \AgdaKeyword{where}\<%
\\
\>[0]\AgdaIndent{2}{}\<[2]%
\>[2]\AgdaInductiveConstructor{\_∷\_} \AgdaSymbol{:} \AgdaBound{A} \AgdaSymbol{→} \AgdaDatatype{∞} \AgdaSymbol{(}\AgdaDatatype{Stream} \AgdaBound{A}\AgdaSymbol{)} \AgdaSymbol{→} \AgdaDatatype{Stream} \AgdaBound{A}\<%
\end{code}

The delay operator $\infty$ denotes an coinductive argument. Given $a:A$, the expression with a delay function $\sharp~a$ is a element of type $\infty~A$. $\beta~x$ will force computation in $x : \infty~A$.

\item If type checker can infer the type for some arguments, we can use implicit arguments which are indicated by curly brackets.
For example a function $f:\{x:A\} \to B$ allows us to omit unnecessary argument which makes the code more readable.




\end{itemize}



\subsection{Identity Type}

Identity type is the type introduced by Martin-L\"{o}f to encode the propositional equality for definitionally equal terms \cite{nor:90}. For any two terms of $\mathsf{a\,b : A}$, we have the type $\mathsf{Id (A , a , b)}$ which is inhabitted when $\mathsf{a}$ and $\mathsf{b}$ are definitionally equal. Here we use an alternative equivalent version named after Paulin-Mohring which is parameterized with the left side of the identity. %This also includes the identity type for arbitrary universe level.

\begin{code}%
\\
\>\AgdaKeyword{data} \AgdaDatatype{\_≡\_} \AgdaSymbol{\{}\AgdaBound{A} \AgdaSymbol{:} \AgdaPrimitiveType{Set}\AgdaSymbol{\}} \AgdaSymbol{(}\AgdaBound{x} \AgdaSymbol{:} \AgdaBound{A}\AgdaSymbol{)} \AgdaSymbol{:} \AgdaBound{A} \AgdaSymbol{→} \AgdaPrimitiveType{Set} \AgdaKeyword{where}\<%
\\
\>[0]\AgdaIndent{2}{}\<[2]%
\>[2]\AgdaInductiveConstructor{refl} \AgdaSymbol{:} \AgdaBound{x} \AgdaDatatype{≡} \AgdaBound{x}\<%
\\
%
\end{code}

In Agda eliminators are not automatically derived for the types defined. Instead we have pattern matching generally which is sometimes stronger than eliminators.
As long as we pattern match on a variable of an identity type with the unique inhabitant $\AgdaInductiveConstructor{refl}$, all occurences of both variables become the same.
It is stronger and it provides the eliminator J.

\begin{code}
%
\\
\>\AgdaFunction{J} \AgdaSymbol{:} \AgdaSymbol{(}\AgdaBound{A} \AgdaSymbol{:} \AgdaPrimitiveType{Set}\AgdaSymbol{)(}\AgdaBound{a} \AgdaSymbol{:} \AgdaBound{A}\AgdaSymbol{)} \AgdaSymbol{→} \AgdaSymbol{(}\AgdaBound{P} \AgdaSymbol{:} \AgdaSymbol{(}\AgdaBound{b} \AgdaSymbol{:} \AgdaBound{A}\AgdaSymbol{)} \AgdaSymbol{→} \AgdaBound{a} \AgdaDatatype{≡} \AgdaBound{b} \AgdaSymbol{→} \AgdaPrimitiveType{Set}\AgdaSymbol{)}\<%
\\
\>[0]\AgdaIndent{2}{}\<[2]%
\>[2]\AgdaSymbol{→} \AgdaBound{P} \AgdaBound{a} \AgdaInductiveConstructor{refl}\<%
\\
\>[0]\AgdaIndent{2}{}\<[2]%
\>[2]\AgdaSymbol{→} \AgdaSymbol{(}\AgdaBound{b} \AgdaSymbol{:} \AgdaBound{A}\AgdaSymbol{)(}\AgdaBound{p} \AgdaSymbol{:} \AgdaBound{a} \AgdaDatatype{≡} \AgdaBound{b}\AgdaSymbol{)} \AgdaSymbol{→} \AgdaBound{P} \AgdaBound{b} \AgdaBound{p}\<%
\\
\>\AgdaFunction{J} \AgdaBound{A} \AgdaSymbol{.}\AgdaBound{b} \AgdaBound{P} \AgdaBound{m} \AgdaBound{b} \AgdaInductiveConstructor{refl} \AgdaSymbol{=} \AgdaBound{m}\<%
\\
%
\end{code}

\subsection{Extensionality}
\label{extensionality}

In regular mathematics, equality does not only exists between intensionally equal terms. Objects are also equal if they have the same extensional external properties, like functions and propositions. 

As Martin Hofmann summarises in \cite{hof:phd}, there are several important extensional concepts which we will also encounter in this thesis, \emph{Functional extensionality}, \emph{Uniqueness of identity}, \emph{Proof-irrelevance}, \emph{Propositional extensionality}, \emph{Quotient types}. 
These principles are not currently available in \itt, but they are valid extensions of Type Theory and worth interpreting to help both Mathematics and programs constructions. Intuitively speaking, the extensionally equal terms can be distingushed by any other terms, hence the extensionality is justifiable. 

\begin{itemize}
\item \textit{Functional extensionality} It has been introduced in \ref{fun-ext}.

\item \textit{Uniqueness of identity} In type theory we define a notion of \emph{set} as follows:

\begin{definition}\label{UIP}
A type $A$ is a \textbf{set} if for all $x,y:A$ and all $p,q:x=y$, we have $p=q$.
\end{definition}

This property is usually called uniqueness of identity (UIP). From the usual definition of identity types, UIP is not a result for every type. UIP is also equivalent to Streicher’s “Axiom K”.

\begin{axiom}[K]
For all $x:A$ and $p: x=x$ we have $p=\text{refl}_{x}$.
\end{axiom}

Another characterisation of a set in type theory is given by Hedberg's Theorem.
\begin{theorem}[Hedberg]
If $A$ has decidable equality, then $A$ is a set.
\end{theorem}

\item \textit{Proof-irrelevance} All proofs of the same proposition are propositionally equal.

\item \textit{Propositional extensionality} Two logically equivalent propositions are propositionally equal.

\item \textit{Quotient types} A quotient type is a type formed by redefing equality on a underlying type with a given equivalence relation on it. 

\item \textit{Univalence Axiom}
In \hott, univalence is a extensional principle which states that ismorphic types are propositionally equal.
\end{itemize}

It is interesting to extend Type Theory with these extensional principles. However, it only makes sense if the type-checking decidability and terms canonicity are not sacrificed. This thesis mainly focus on the extension of quotient types.


\section{Homotopy Type Theory}

\hott is a variant of intensional \mltt{} which is a new branch developed between theoretical computer science
and mathematics. Vladimir Voevodsky found a surprising connection between homotopy theory and type theory \cite{voe:06}. He proposed the univalence axiom, which identifies isomorphic structures, as a univalent foundation for mathematics. 
In \hott, there is an observation that notions in type theory can be interpreted by homotopy-theoretical terms. A type is regarded as a \emph{space} and a term of this type is a \emph{point} of this space. Functions between types are \emph{continous maps} and identity types are usually considered as \emph{paths}. Identity types of identity types are \emph{homotopies}. Although these notions are originally defined with topological bases, we only employ them as homotopical notions on a higher level. 


As univalence axiom states, equality is equivalent to equivalence. Acutally it can be seen as an formal acceptance of the ''common sense'' in Mathematics that isomorphic structures can be identitfied. The higher structures of the equivalence also allows us to study the different ways of identification. Therefore it is more appropriate to interpret types as higher groupoids. People is trying to implement \hott in \itt and one possible way is to interpret \wog first in Agda. The author has done some work in this direction which can be found in Chapter~\ref{wog}.


I will not explain this topic in detail here, but in \autoref{HITs}
we give a more detailed introduction to it.



 % Something without a type makes no sense to us because we are not sure what it stands for and how do we use it. The type definition describe the syntax so that some symbol makes sense and the semantic meaning may be revealed from the construction.

% There is another question, whether mathematics is a collection of patterns and laws which is observed, or it is a system created and built by people to explain the patterns and laws in the world. I think people prefer the second answer usually accept the type theory more easily, although most people (probably 99.9 percent) prefer the first one. When we learn what is natural numbers, we learn it as "numbers like 1, 2, 3 ,4 and perhaps 0", the commutative law, associate law are axioms because there is no way to prove it if we introduce it in this manner. We are convinced by some examples like "2 + 3 = 3 + 2" and we find it works for most of the cases then we accept it by observations. It is some methods physicians used a lot -- to conclude some laws from a number of facts. It is a proper method for physicians because what they research on is world can only be observed. However for mathematics, even though it is applied to the real world, it is a system completely created by people. People used their fingers to count, wrote symbols for results, even though it was very shallow it is obviously a aritificial system. People extend 


% Type theory is strongly connected with computation theory.
% Set


% Type theory has fewer axioms, simpler model than set theory which has mutual foudations: logic and axioms.

\section{Summary}


In mathematics, set theory is still a more popular choice over type theory. However in computer science, instuitionistic type theory is worth more studying. It is more close to program construction and from a computer scientist's point of view, it is very natural to accept intuitionistic logic. 

It provides a foundation of mathematics which can be implemented as a programming language so that proving is just programming and verification is just type checking. The aid of computers saves a lot of work from mathematicians and reduces the chance of making mistakes, although the absence of \emph{principle of excluded middle} in intuitionistic logic makes some mathematicians hard to accept. There is a very good talk given by Andrej Bauer in IAS called "Five Stages of Accepting Constructive Mathematics'' online \footnote{available on Youtube}.




